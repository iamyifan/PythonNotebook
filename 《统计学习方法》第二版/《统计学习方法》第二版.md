# 《统计学习方法》第二版

[TOC]

## 第 1 篇	监督学习

### 第 1 章	统计学习及监督学习概论

**监督学习**是从**标注数据**中**学习模型**的机器学习问题，是统计学习或机器学习的重要组成部分。

- 1.1 节叙述统计学习或机器学习的定义
- 1.2 节叙述统计学习的分类
- 1.3 节叙述统计学习方法的三要素：模型、策略和算法
- 1.4 - 1.7 节介绍监督学习的几个重要概念，包括模型评估与模型选择
- 1.8 节介绍监督学习的应用：分类问题，标注问题与回归问题

#### 1.1 统计学习

##### 1. 统计学习的特点

统计学习（Statistical Learning）又称统计机器学习（Statistical Machine Learning），是关于计算机基于数据构建概率统计模型，并运用模型对数据进行预测与分析的一门学科。

主要特点：

- 以计算机及网络为平台，建立在计算机及网络上
- 以数据为研究对象，是数据驱动的学科
- 目的是对数据进行预测与分析
- 以方法为中心，统计学习方法构建模型，并应用模型进行预测与分析
- 概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，在发展中逐步形成独自的理论体系与方法论

统计学习是计算机系统通过运用数据及统计方法提高系统性能的机器学习。

##### 2. 统计学习的对象

统计学习的对象是数据（Data），包括了各种数字、文字、图像、视频、音频数据以及它们的结合。

统计学习从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，最后回到对数据的分析与预测。

统计学习的基本假设和前提是同类数据具有一定的统计规律性，同类数据指具有某种共同性质的数据。

在统计学习中，以变量和变量组表示数据，类型可分为连续变量和离散变量。

##### 3. 统计学习的目的

统计学习用于对数据的预测与分析，特别是对未知新数据的预测与分析。总的目标是考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时也要考虑尽可能地提高学习效率。

##### 4. 统计学习的方法

统计学习由监督学习（Supervised Learning）、无监督学习（Unsupervised Learning）和强化学习（Reinforcement Learning）等组成。

统计学习方法概括如下：从给定的、有限的、用于学习的训练数据（Training Set）集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间（Hypothesis Space）；应用某个评价准则（Evaluation Criterion），从假设空间中选取一个最优模型，使它对已知的训练数据集及未知的测试数据（Test Data）在给定的评价准则下有最优的预测；最优模型的选取由算法实现。

因此，统计学习方法包括了模型的假设空间、模型选择的准则和模型学习的算法，简称为模型（Model）、策略（Strategy）和算法（Algorithm）。

实现统计学习方法的步骤如下：

1. 获得一个有限的训练数据集合；
2. 确定模型的假设空间，即学习模型的集合；
3. 确定模型选择的准则，即学习的策略；
4. 实现求解最优模型的算法，即学习的算法；
5. 通过学习方法选择最优模型；
6. 利用学习的最优模型对新数据进行预测或分析。

##### 5. 统计学习的研究

统计学习研究一般包括：

- 统计学习方法：开发新的学习方法；
- 统计学习理论：探求统计学习方法的有效性和效率，以及统计学习的基本理论问题；
- 统计学习应用：将统计学习方法应用到实际问题中，解决实际问题。

##### 6. 统计学习的重要性

统计学习已被成功应用到人工智能、模式识别、数据挖掘、自然语言处理、语音处理、计算视觉、信息检索、生物信息等许多计算机应用领域中，并且成为这些领域的核心技术。

重要性主要体现在以下几个方面：

- 统计学习是处理海量数据的有效方法；

- 统计学习是计算机智能化的有效手段；

- 统计学习是计算机科学发展的一个重要组成部分。

#### 1.2 统计学习的分类

##### 1.2.1 基本分类

统计学习一般包括监督学习、无监督学习、强化学习、半监督学习和主动学习等。

1. 监督学习（Supervised Learning）

   监督学习指从标注数据中学习预测模型的机器学习问题。标注数据表示输入输出的对应关系，预测模型对给定的输入产生相应的输出。监督学习的本质是学习输入到输出的映射的统计规律。

   - 输入空间、特征空间和输出空间

     输入空间（Input Space）：所有可能的输入值取值集合。

     输出空间（Output Space）：所有可能的输出值的取值集合。

     输入与输出空间可以是有限元素的集合，也可以是整个欧氏空间。输入与输出空间可以是同一个空间，也可以是不同的空间，通常输出空间远远小于输入空间。

     特征空间（Feature Space）：所有特征向量（Feature Vector）存在的空间。

     每个具体的输入是一个实例（Instance），通常由特征向量表示。特征向量的每一维度对应于一个特征。

     有时对输入空间与特征空间为相同的空间；有时假设输入空间与特征空间为不同的空间，需要将实例从输入空间映射到特征空间。模型实际上定义在特征空间上。

     输入与输出可以看作是定义在输入（特征）空间与输出空间上的随机变量的取值。输入通常表示为 $X$，输出通常表示为 $Y$。输入变量的取值写作 $x$，输出变量的取值写作 $y$。变量可以是标量或向量，通常默认为列向量。输入实例 $x$ 的特征向量记作
     $$
     x_i = (x_i^{(1)}, x_i^{(2)}, …, x_i^{(k)}, …, x_i^{(n)})^T
     $$
     $x_i^{(k)}$ 表示第 $i$ 个实例的第 $k$ 个特征。
     
     监督学习从训练数据（Training Data）集合中学习模型，对测试数据（Test Data）进行预测。
     
     训练数据由输入（或特征向量）与输出对组成，通常表示为
     $$
     T = \{(x_1, y_1), (x_2, y_2), …, (x_N, y_N)\}
     $$
     测试数据也由输入和输出对组成。输入与输出对又称为样本（Sample）或样本点。
     
     输入变量 $X$ 和输出变量 $Y$ 可分为离散型和连续性。回归问题指输入变量与输出变量均为连续变量的预测问题；分类问题指输出变量为有限个离散变量的预测问题；标注问题指输入变量与输出变量均为变量序列的预测问题。

   - 联合概率分布

     监督学习假设输入与输出的随机变量 $X$ 和 $Y$ 遵循联合概率分布 $P(X, Y)$。在学习过程中，假定这一联合概率分布存在，但具体定义未知。训练数据与测试数据被看作是依联合概率分布 $P(X, Y)$ 独立同分布产生的。$X$ 和 $Y$ 具有联合概率分布 $P(X, Y)$ 是监督学习关于数据的基本假设。

   - 假设空间（Hypothesis Space）

     监督学习的目的在于学习一个由输入到输出的映射，并通过模型表示。假设空间包含了由输入空间到输出空间的映射（模型）的集合，假设空间确定了学习的范围。

     监督学习的模型可以是概率模型或非概率模型。概率模型由条件概率分布 $P(Y|X)$ 表示，非概率模型由决策函数（Decision Function）$Y = f(X)$ 表示。对具体的输入进行相应的输出预测时，写作 $P(y|x)$ 或 $y=f(x)$。

   - 问题的形式化

     给定训练数据集
     $$
     T = \{(x_1, y_1), (x_2, y_2), … , (x_N, y_N)\}
     $$
     其中，$(x_i, y_i), i = 1, 2, …, N$，称为样本或样本点。$x_i \in \mathcal{X} \subseteq \boldsymbol{R^n} $ 是输入的观测值，也称为输入或实例。$y \in \mathcal{Y}$ 是输出的观测值，也成为输出。

     监督学习分为学习和预测两个过程，由学习系统和预测系统完成。

     在学习过程中，学习系统通过利用给定的训练数据集，通过学习（或训练）得到一个模型，表示为条件概率分布 $\hat{P}(Y|X)$ 或决策函数 $Y=\hat{f}(X)$，二者描述了输入与输出随机变量之间的映射关系。在预测过程中，预测系统对于给定的预测样本集中的输入 $X_{N+1}$，由模型 $y_{N+1} = \arg \max\limits_{y} \hat{P}(y|x_{N+1})$ 或 $y_{N+1}=\hat{f}(x_{N+1})$ 给出相应的输出 $y_{N+1}$。

     在监督学习中，假定训练数据与测试数据是依联合概率分布 $P(X, Y)$ 独立同分布产生的。

     学习系统（算法）通过训练数据集中的样本 $(x_i, y_i)$ 带来的信息学习模型。学习系统通过不断尝试，选取最好的模型，以便对训练数据集有足够好的预测，同时对未知的测试数据集的预测也有尽可能好的推广。

2. 无监督学习

3. 强化学习

4. 半监督学习与主动学习







#### 1.3 统计学习方法三要素





#### 1.4 模型评估与模型选择





#### 1.5 正则化与交叉验证





#### 1.6 泛化能力





#### 1.7 生成模型与判别模型





#### 1.8 监督学习的应用





## 第 2 篇	无监督学习

### 第 14 章	聚类方法

聚类，指根据给定的样本，依据它们特征的相似度或距离，将其归并到若干个“类”或“簇”的数据分析问题。一个类是给定样本集合的一个子集。直观上，相似的样本聚集在相同的类，不相似的样本分散在不同的类。

聚类的目的是通过得到的类或簇来发现数据的特点或对数据进行处理。聚类属于无监督学习，因其只根据样本的相似度或距离将其进行归类，而类或簇事先并不知道。

#### 14.1	聚类的基本概念

##### 14.1.1	相似度或距离

假设有 $n$ 个样本，每个样本由 $m$ 个属性特征向量组成。样本集合 $X$ 表示如下：
$$
X = [x_{ij}]_{m \times n} =
\begin{bmatrix}
x_{11} & x_{12} & \dots & x_{1n} \\
x_{21} & x_{22} & \dots & x_{2n} \\
 \vdots & \vdots & & \vdots \\
x_{m1} & x_{m2} & \dots & x_{mn}
\end{bmatrix}
$$
其中，第 $j$ 列表示第 $j$ 个样本，$j = 1, 2, \dots, n$；第 $i$ 行表示第 $i$ 个属性，$i = 1, 2, \dots, m$；矩阵元素 $x_{ij}$ 表示第 $j$ 个样本的第 $i$ 个属性值，$i = 1, 2, \dots, m; j = 1, 2, \dots, n$。

注意，在聚类中使用不同的相似度和距离定义有可能会导致不同的结果。

1. 闵可夫斯基距离（Minkowski Distance）

   给定样本集合 $X, X \subseteq \boldsymbol{R^m}$，其中，$x_i, x_j \in X, x_i = (x_{1i}, x_{2i}, \dots, x_{mi}) ^ T, x_j = (x_{1j}, x_{2j}, \dots, x_{mj}) ^ T$。

   样本 $x_i$ 和 $x_j$ 的闵可夫斯基距离定义为：
   $$
   d_{ij} = \left( \sum_{k=1}^m |x_{ki} - x_{kj}|^p \right)^{\frac{1}{p}}, p \ge 1
   $$
   当 $p = 1$ 时，称为曼哈顿距离（Manhattan Distance）：
   $$
   d_{ij} = \sum_{k=1}^m |x_{ki} - x_{kj}|
   $$
   当 $p = 2$ 时，称为欧氏距离（Euclidean Distance）：
   $$
   d_{ij} = \left( \sum_{k=1}^m |x_{ki} - x_{kj}|^2 \right)^{\frac{1}{2}}
   $$
   当 $p = \infin$ 时，称为切比雪夫距离（Chebyshev Distance），即所有特征差值的绝对值的最大值：
   $$
   d_{ij} = \max_{k = 1, 2, \dots, m} |x_{ki} - x_{kj}|
   $$
   闵可夫斯基距离越小，相似度越大；距离越大，相似度越小。

2. 马哈拉诺比斯距离（Mahalanobis Distance）

   给定样本集合 $X, X \subseteq \boldsymbol{R^m}$，其中，$x_i, x_j \in X, x_i = (x_{1i}, x_{2i}, \dots, x_{mi}) ^ T, x_j = (x_{1j}, x_{2j}, \dots, x_{mj}) ^ T$。

   样本 $x_i$ 和 $x_j$ 的马哈拉诺比斯距离（马氏距离）定义如下：
   $$
   d_{ij} = [(x_i - x_j)^T S^{-1} (x_i - x_j)]^{\frac{1}{2}}
   $$
   其中，$S$ 是 $X$ 的协方差矩阵。当 $S$ 是单位矩阵时，即样本数据的各个分量互相独立且各个分量的方差为 $1$ 时，马氏距离即为欧氏距离。马氏距离越大，相似度越小；距离越小，相似度越大。

3. 相关系数（Correlation Coefficient）

   给定样本集合 $X, X \subseteq \boldsymbol{R^m}$，其中，$x_i, x_j \in X, x_i = (x_{1i}, x_{2i}, \dots, x_{mi}) ^ T, x_j = (x_{1j}, x_{2j}, \dots, x_{mj}) ^ T$。

   样本 $x_i$ 和 $x_j$ 的相关系数定义如下：
   $$
   r_{ij} = \frac
   {\displaystyle \sum_{k=1}^m (x_{ki} - \bar{x}_i) (x_{kj} - \bar{x}_j)}
   {\left[ \displaystyle \sum_{k=1}^m(x_{ki} - \bar{x}_i)^2 \sum_{k=1}^m(x_{kj} - \bar{x}_j)^2\right]^\frac{1}{2}}
   $$
   其中，
   $$
   \bar{x}_i = \frac{1}{m} \sum_{k=1}^m x_{ki}, \bar{x}_j = \frac{1}{m} \sum_{k=1}^m x_{kj}
   $$
   相关系数的绝对值越接近于 $1$，越相似；越接近于 $0$，越不相似。

4. 夹角余弦（Cosine）

   给定样本集合 $X, X \subseteq \boldsymbol{R^m}$，其中，$x_i, x_j \in X, x_i = (x_{1i}, x_{2i}, \dots, x_{mi}) ^ T, x_j = (x_{1j}, x_{2j}, \dots, x_{mj}) ^ T$。

   样本 $x_i$ 和 $x_j$ 的夹角余弦定义如下：
   $$
   s_{ij} = \frac
   {\displaystyle \sum_{k=1}^m x_{ki} x_{kj}}
   {\displaystyle \left[ \sum_{k=1}^m x_{ki}^2 \sum_{k=1}^m x_{kj}^2 \right]^\frac{1}{2}}
   $$
   夹角余弦的值越接近于 $1$，越相似；越接近于 $0$，越不相似。
   
##### 14.1.2	类或簇

通过聚类得到的类或簇，本质上是样本的子集。根据类或簇中元素的关系，定义硬分类（Hard Clustering）和软分类（Soft Clustering）如下：

- 硬分类：聚类方法假定一个样本只能属于一个类，或类的交集为空集；
- 软分类：聚类方法假定一个样本可以属于多个类，或类的交集不为空集。

定义 $G$ 表示类或簇，$x_i, x_j$ 表示类中的样本，$n_G$ 表示 $G$ 中样本的个数，$d_{ij}$ 表示样本 $x_i$ 和 $x_j$ 的距离。

几种常见的类或簇的定义如下：

1.  设 $T$ 为给定的正数，若集合 $G$ 中任意两个样本 $x_i, x_j$，有
   $$
   d_{ij} < T
   $$
   则称 $G$ 为一个类或簇。

2.  设 $T$ 为给定的正数，若对集合 $G$ 中任意一个样本 $x_i$，一定存在 $G$ 中另一个样本 $x_j$，使得
   $$
   d_{ij} < T
   $$
   则称 $G$ 为一个类或簇。

3.  设 $T$ 为给定的正数，若对集合 $G$ 中任意一个样本 $x_i$，$G$ 中其余样本 $x_j$ 满足
   $$
   \frac{1}{n_G - 1} \sum_{x_j \in G}d_{ij} \le T
   $$
   则称 $G$ 为一个类或簇。

4. 设 $T$ 和 $V$ 是为给定的两个正数，如果集合 $G$ 中任意两个样本对 $x_i, x_j$ 之间的距离 $d_{ij}$ 满足
   $$
   \frac{1}{n_G (n_G - 1)} \sum_{x_i \in G} \sum_{x_j \in G} d_{ij} \le T \\
   d_{ij} \le V
   $$
   则称 $G$ 为一个类或簇。

几种常用的类的特征定义如下：

1. 类的均值 $\bar{x}_G$，即类的中心
   $$
   \bar{x}_G = \frac{1}{n_G} \sum_{i=1}^{n_G} x_i
   $$
   $n_G$ 为类 $G$ 中的样本数量。

2. 类的直径（Diameter）$D_G$
   $$
   D_G = \max_{x_i, x_j \in G} d_{ij}
   $$
   即类中任意两个样本之间距离的最大值。

3. 类的样本散布矩阵（Scatter Matrix）$A_G$ 与样本协方差矩阵（Covariance Matrix）$S_G$
   $$
   A_G = \sum_{i=1}^{n_G} (x_i-\bar{x}_G)(x_i - \bar{x}_G)^T
   $$

   $$
   S_G  = \frac{1}{m-1}A_G
   $$

   $m$ 为样本的维度，即样本属性的个数。

##### 14.1.3	类与类之间的距离

类 $G_p$ 与 $G_q$ 之间的距离 $D(p, q)$ 也称为连接（linkage）。设 $G_p$ 包含 $n_p$ 个样本，中心表示为 $\bar{x}_p$；$G_q$ 包含 $n_q$ 个样本，中心表示为 $\bar{x}_q$。常见的几种连接定义如下：

1. 最短距离或单连接（Single Linkage）
   $$
   D_{pq} = \min \{d_{ij} | x_i \in G_p, x_j \in G_q\}
   $$
   即类 $G_p$ 中的样本和 $G_q$ 中的样本之间距离的最小值。

2. 最长距离或完全连接（Complete Linkage）
   $$
   D_{pq} = \max \{d_{ij} | x_i \in G_p, x_j \in G_q\}
   $$
   即类 $G_p$ 中的样本和 $G_q$ 中的样本之间距离的最大值。

3. 中心距离
   $$
   D_{pq} = d_{\bar{x}_p, \bar{x}_q}
   $$
   即类 $G_p$ 的样本中心和 $G_q$ 的样本中心之间的距离。

4. 平均距离
   $$
   D_{pq} = \frac{1}{n_p n_q} \sum_{x_i \in G_p} \sum_{x_j \in G_q} d_{ij}
   $$
   即类 $G_p$ 中的样本和 $G_q$ 中的样本之间距离的平均值。

#### 14.2	层次聚类

#### 14.3	$k$ 均值聚类

##### 14.3.1	模型

##### 14.3.2	策略

##### 14.3.3	算法

##### 14.3.4	算法特性



